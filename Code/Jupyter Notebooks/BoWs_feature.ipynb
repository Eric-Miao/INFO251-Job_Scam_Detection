{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc28b031-13d2-4e08-9d55-e4a0783006db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt  \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cf425f8-20ec-4854-92f8-564fc20c744f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>food52 created groundbreaking award winning co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90 seconds worlds cloud video production servi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>valor services provides workforce solutions me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>passion improving quality life geography heart...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spotsource solutions llc global human capital ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>vend looking awesome new talent come join us w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17876</th>\n",
       "      <td>weblinc e commerce platform services provider ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17877</th>\n",
       "      <td>provide full time permanent positions many med...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>nemsia studios looking experienced visual grap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17879</th>\n",
       "      <td>vend looking awesome new talent come join us w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17880 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  fraudulent\n",
       "0      food52 created groundbreaking award winning co...           0\n",
       "1      90 seconds worlds cloud video production servi...           0\n",
       "2      valor services provides workforce solutions me...           0\n",
       "3      passion improving quality life geography heart...           0\n",
       "4      spotsource solutions llc global human capital ...           0\n",
       "...                                                  ...         ...\n",
       "17875  vend looking awesome new talent come join us w...           0\n",
       "17876  weblinc e commerce platform services provider ...           0\n",
       "17877  provide full time permanent positions many med...           0\n",
       "17878  nemsia studios looking experienced visual grap...           0\n",
       "17879  vend looking awesome new talent come join us w...           0\n",
       "\n",
       "[17880 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./../../Data/text.csv')\n",
    "df.fraudulent=df.fraudulent.replace('f',0)\n",
    "df.fraudulent=df.fraudulent.replace('t',1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f9aa2-1f52-4a63-9336-5edf7a4a3578",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Bag of Words using Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b38bf42-6ef5-4ce6-8376-71febaebc0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_in_text function that would calculatethe frequnecy of the word\n",
    "def words_in_texts(words, texts):\n",
    "    indicator_array = 1 * np.array([texts.str.contains(word) for word in words]).T\n",
    "    return indicator_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a97bb594-84f7-4b2e-92c7-ac85bcc8aeba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/4tc3ywx9067g4762nwshwf000000gn/T/ipykernel_49710/470834598.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  ham_split = eda_ham['text'].str.replace(r'/<[^>]*>/g', ' ').str.split()\n",
      "/var/folders/29/4tc3ywx9067g4762nwshwf000000gn/T/ipykernel_49710/470834598.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  spam_split = eda_spam['text'].str.replace(r'/<[^>]*>/g', ' ').str.split()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9728\n"
     ]
    }
   ],
   "source": [
    "# Create bag of word dictionary\n",
    "\n",
    "eda_ham = df.loc[df['fraudulent']==0]\n",
    "eda_spam = df.loc[df['fraudulent']==1]\n",
    "\n",
    "num_ham, num_spam = {}, {}\n",
    "\n",
    "ham_split = eda_ham['text'].str.replace(r'/<[^>]*>/g', ' ').str.split()\n",
    "spam_split = eda_spam['text'].str.replace(r'/<[^>]*>/g', ' ').str.split()\n",
    "\n",
    "#put word frequencies in dictionaries\n",
    "for i in ham_split:\n",
    "    for j in i:\n",
    "        if num_ham.get(j) is None:\n",
    "            num_ham[j] = 1\n",
    "        num_ham[j] = num_ham[j] + 1\n",
    "for i in spam_split:\n",
    "    for j in i:\n",
    "        if num_spam.get(j) is None:\n",
    "            num_spam[j] = 1\n",
    "        num_spam[j] = num_spam[j] + 1\n",
    "        \n",
    "#sorted_ham = sorted(num_ham, key = num_ham.get, reverse = True)\n",
    "sorted_spam = sorted(num_spam, key = num_spam.get, reverse = True)\n",
    "print(len(sorted_spam)) #95402 pairs in the dictionary\n",
    "\n",
    "# appended the list of words by the number occurences in the spam set \n",
    "feature = []\n",
    "for i in np.arange(1200):\n",
    "    feature.append(sorted_spam[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2500075-2597-4ede-b3f7-31abbcdaa72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 words\n",
      "Accuracy score: 0.9496644295302014\n",
      "Precision score: 0.017937219730941704\n",
      "Recall score: 0.4\n",
      "F1 score: 0.034334763948497854\n",
      "100 words\n",
      "Accuracy score: 0.9579418344519016\n",
      "Precision score: 0.242152466367713\n",
      "Recall score: 0.7397260273972602\n",
      "F1 score: 0.36486486486486486\n",
      "200 words\n",
      "Accuracy score: 0.9635346756152126\n",
      "Precision score: 0.3901345291479821\n",
      "Recall score: 0.7631578947368421\n",
      "F1 score: 0.5163204747774481\n",
      "300 words\n",
      "Accuracy score: 0.9691275167785235\n",
      "Precision score: 0.5112107623318386\n",
      "Recall score: 0.7972027972027972\n",
      "F1 score: 0.6229508196721312\n",
      "400 words\n",
      "Accuracy score: 0.9713646532438479\n",
      "Precision score: 0.547085201793722\n",
      "Recall score: 0.8187919463087249\n",
      "F1 score: 0.6559139784946236\n",
      "500 words\n",
      "Accuracy score: 0.9727069351230425\n",
      "Precision score: 0.6457399103139013\n",
      "Recall score: 0.7700534759358288\n",
      "F1 score: 0.7024390243902439\n",
      "600 words\n",
      "Accuracy score: 0.9751677852348993\n",
      "Precision score: 0.6771300448430493\n",
      "Recall score: 0.7947368421052632\n",
      "F1 score: 0.7312348668280871\n",
      "700 words\n",
      "Accuracy score: 0.9749440715883669\n",
      "Precision score: 0.6681614349775785\n",
      "Recall score: 0.7967914438502673\n",
      "F1 score: 0.726829268292683\n",
      "800 words\n",
      "Accuracy score: 0.9758389261744966\n",
      "Precision score: 0.6771300448430493\n",
      "Recall score: 0.8074866310160428\n",
      "F1 score: 0.7365853658536585\n",
      "900 words\n",
      "Accuracy score: 0.9787472035794184\n",
      "Precision score: 0.7130044843049327\n",
      "Recall score: 0.8368421052631579\n",
      "F1 score: 0.7699757869249394\n",
      "1000 words\n",
      "Accuracy score: 0.978076062639821\n",
      "Precision score: 0.7130044843049327\n",
      "Recall score: 0.8238341968911918\n",
      "F1 score: 0.764423076923077\n",
      "1100 words\n",
      "Accuracy score: 0.9787472035794184\n",
      "Precision score: 0.726457399103139\n",
      "Recall score: 0.826530612244898\n",
      "F1 score: 0.7732696897374702\n",
      "1200 words\n",
      "Accuracy score: 0.9782997762863535\n",
      "Precision score: 0.7174887892376681\n",
      "Recall score: 0.8247422680412371\n",
      "F1 score: 0.7673860911270982\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['fraudulent'], test_size = 0.25, random_state = 42) \n",
    "\n",
    "num_words = [50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]\n",
    "\n",
    "accuracy_score_lst = []\n",
    "precision_score_lst = []\n",
    "recall_score_lst = []\n",
    "f1_score_lst = []\n",
    "\n",
    "for num in num_words:\n",
    "    arr_feature = feature[:num]\n",
    "    \n",
    "    train_X = words_in_texts(arr_feature, X_train)\n",
    "    train_Y = np.array(y_train)\n",
    "    \n",
    "    test_x = words_in_texts(arr_feature, X_test)\n",
    "    test_y = np.array(y_test)\n",
    "    \n",
    "    model_eda = LogisticRegression(max_iter=10000)\n",
    "    model_eda.fit(train_X, train_Y)\n",
    "\n",
    "    pred = model_eda.predict(test_x)\n",
    "    new_training_accuracy = model_eda.score(train_X, train_Y)\n",
    "\n",
    "    # print(\"Accuracy: \", new_training_accuracy)\n",
    "    print(num, 'words')\n",
    "    print('Accuracy score:' , accuracy_score(pred, y_test))\n",
    "    print('Precision score:', precision_score(pred, y_test))\n",
    "    print ('Recall score:', recall_score(pred, y_test))\n",
    "    print ('F1 score:', f1_score(pred, y_test))\n",
    "    \n",
    "    accuracy_score_lst.append(accuracy_score(pred, y_test))\n",
    "    precision_score_lst.append(precision_score(pred, y_test))\n",
    "    recall_score_lst.append(recall_score(pred, y_test))\n",
    "    f1_score_lst.append(f1_score(pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d6c1318-9757-43bc-b50f-181a9ab7de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Accuracy  Precision  Recall     F1\n",
      "50       0.950      0.018   0.400  0.034\n",
      "100      0.958      0.242   0.740  0.365\n",
      "200      0.964      0.390   0.763  0.516\n",
      "300      0.969      0.511   0.797  0.623\n",
      "400      0.971      0.547   0.819  0.656\n",
      "500      0.973      0.646   0.770  0.702\n",
      "600      0.975      0.677   0.795  0.731\n",
      "700      0.975      0.668   0.797  0.727\n",
      "800      0.976      0.677   0.807  0.737\n",
      "900      0.979      0.713   0.837  0.770\n",
      "1000     0.978      0.713   0.824  0.764\n",
      "1100     0.979      0.726   0.827  0.773\n",
      "1200     0.978      0.717   0.825  0.767\n"
     ]
    }
   ],
   "source": [
    "top_words_result = pd.DataFrame({'Accuracy':accuracy_score_lst,'Precision':precision_score_lst,\n",
    "                                 'Recall':recall_score_lst,'F1':f1_score_lst,}, index=num_words)\n",
    "print(top_words_result.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987ceb8-4409-4f95-aed9-541259c71eed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Bag of Words using CountVectorizer (all words in Training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "fb986a83-34da-4a9f-84fc-2a79a49f92aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17014\n",
       "1      866\n",
       "Name: fraudulent, dtype: int64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fraudulent']=df['fraudulent'].replace('f', 0)\n",
    "df['fraudulent']=df['fraudulent'].replace('t', 1)\n",
    "df['fraudulent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0a8609d9-b032-495b-85f5-0981c21a52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d5e7b649-1116-47ea-a859-cefd6197eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train and Testing Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['fraudulent'], test_size = 0.25, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "613cf96d-f7a6-4804-aaa9-299f57b9c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that would train and predict testing data.\n",
    "# Takes in x, y for train and test set, and a tuple for ngram. \n",
    "def count_vect(x_train, x_test, Y_train, Y_test, ngram):\n",
    "    count_vector = CountVectorizer(ngram_range= ngram, lowercase = True , stop_words =  'english')\n",
    "\n",
    "    X_train = count_vector.fit_transform(x_train) \n",
    "    X_test = count_vector.transform(x_test)\n",
    "    \n",
    "    logitic_regression = LogisticRegression(random_state=42, max_iter=10000)\n",
    "    logitic_regression.fit(X_train , Y_train)\n",
    "\n",
    "    predictions_logistic_regression = logitic_regression.predict(X_test)\n",
    "    \n",
    "    print('Accuracy score:' , accuracy_score(predictions_logistic_regression, y_test))\n",
    "    print('Precision score:', precision_score(predictions_logistic_regression, y_test))\n",
    "    print ('Recall score:', recall_score(predictions_logistic_regression, y_test))\n",
    "    print ('F1 score:', f1_score(predictions_logistic_regression, y_test))\n",
    "    \n",
    "    unique_elements, counts_elements = np.unique(predictions_logistic_regression, return_counts=True)\n",
    "    print(\"Frequency of unique values of the said array:\")\n",
    "    print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "1129999f-8d89-459d-bb83-4b32e73279de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9861297539149888\n",
      "Precision score: 0.7623318385650224\n",
      "Recall score: 0.9497206703910615\n",
      "F1 score: 0.845771144278607\n",
      "Frequency of unique values of the said array:\n",
      "[[   0    1]\n",
      " [4291  179]]\n"
     ]
    }
   ],
   "source": [
    "# n-gram = 1,1\n",
    "count_vect(X_train, X_test, y_train, y_test, (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "92dff825-fb50-4d1a-b223-3f8ef6c9d8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9870246085011186\n",
      "Precision score: 0.7488789237668162\n",
      "Recall score: 0.9881656804733728\n",
      "F1 score: 0.8520408163265306\n",
      "Frequency of unique values of the said array:\n",
      "[[   0    1]\n",
      " [4301  169]]\n"
     ]
    }
   ],
   "source": [
    "# n-gram = 1,2\n",
    "count_vect(X_train, X_test, y_train, y_test, (1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "0754a86b-b56f-4994-b7e8-9fbae93dd545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9841163310961969\n",
      "Precision score: 0.6816143497757847\n",
      "Recall score: 1.0\n",
      "F1 score: 0.8106666666666666\n",
      "Frequency of unique values of the said array:\n",
      "[[   0    1]\n",
      " [4318  152]]\n"
     ]
    }
   ],
   "source": [
    "# n-gram = 2,2\n",
    "count_vect(X_train, X_test, y_train, y_test, (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f05b2b3-b4ab-4fff-88f1-2f5fb80d9046",
   "metadata": {},
   "source": [
    "# Feature Engineering Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "121f78c2-0022-4504-b24d-259a7d5abb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv('emscad_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d2a6b0dc-cb57-409a-82a6-355c9e8d475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn, forest, trees\n",
    "#company_profile\tdescription\trequirements\tbenefits\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "970c6e01-58bd-4036-ba55-da5935b8ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in the original dataset dataframe and conducts feature engineering on the text\n",
    "# evaluates the number of characters, words, and occurences of specific symbols. \n",
    "\n",
    "def method_one_text_features(df2, train_or_test):\n",
    "    df_new = df2.copy()\n",
    "    df_new['company_profile'] = df_new['company_profile'].str.lower()\n",
    "    df_new['company_profile_count_chr'] = df_new['company_profile'].str.len()\n",
    "    df_new['company_profile_count_word'] = df_new['company_profile'].str.split(' ').str.len()\n",
    "    \n",
    "    df_new['description'] = df_new['description'].str.lower()\n",
    "    df_new['description_profile_count_chr'] = df_new['description'].str.len()\n",
    "    df_new['description_profile_count_word'] = df_new['company_profile'].str.split(' ').str.len()\n",
    "    \n",
    "    df_new['requirements'] = df_new['requirements'].str.lower()\n",
    "    df_new['requirements_count_chr'] = df_new['requirements'].str.len()\n",
    "    df_new['requirements_profile_count_word'] = df_new['company_profile'].str.split(' ').str.len()\n",
    "    \n",
    "    df_new['benefits'] = df_new['benefits'].str.lower()\n",
    "    df_new['benefits_count_chr'] = df_new['benefits'].str.len()\n",
    "    df_new['benefits_profile_count_word'] = df_new['company_profile'].str.split(' ').str.len()\n",
    "\n",
    "    df_new['excl_count'] = df_new['company_profile'].str.count('!') + df_new['description'].str.count('!') + df_new['requirements'].str.count('!') + df_new['benefits'].str.count('!')\n",
    "    df_new['q_count'] =  df_new['company_profile'].str.count('\\?') + df_new['description'].str.count('\\?') + df_new['requirements'].str.count('\\?') + df_new['benefits'].str.count('\\?') \n",
    "    df_new['hash_count'] = df_new['company_profile'].str.count('#') + df_new['description'].str.count('#') + df_new['requirements'].str.count('#') + df_new['benefits'].str.count('#')\n",
    "    df_new['dollar_count'] = df_new['company_profile'].str.count('$') + df_new['description'].str.count('$') + df_new['requirements'].str.count('$') + df_new['benefits'].str.count('$')\n",
    "    df_new['newline_count'] = df_new['company_profile'].str.count('\\n') + df_new['description'].str.count('\\n') + df_new['requirements'].str.count('\\n') + df_new['benefits'].str.count('\\n')\n",
    "    df_new['bracket_count'] =  df_new['company_profile'].str.count(r'\\<.*\\>') + df_new['description'].str.count(r'\\<.*\\>') + df_new['requirements'].str.count(r'\\<.*\\>') + df_new['benefits'].str.count(r'\\<.*\\>')\n",
    "\n",
    "    df_new = df_new.fillna(0)\n",
    "\n",
    "    # dummy_df = pd.get_dummies(data=df_new, columns=['telecommuting','has_company_logo', 'has_questions','employment_type','required_experience','required_education','industry','function'])\n",
    "    # df_new = pd.concat([df_new.drop(columns=['telecommuting','has_company_logo', 'has_questions','employment_type','required_experience', 'required_education','industry','function']), dummy_df], axis=1)\n",
    "    # print(list(df_new.columns))\n",
    "    \n",
    "    if train_or_test == 'train':\n",
    "        df_new = df_new.drop(['telecommuting', 'has_company_logo', 'has_questions', 'employment_type',\n",
    "       'required_experience', 'required_education', 'industry', 'function','title', 'location', 'department', 'salary_range', 'company_profile', 'description', 'requirements', 'benefits', 'in_balanced_dataset',\n",
    "                             'company_profile_count_chr', 'description_profile_count_chr', 'requirements_count_chr', 'benefits_count_chr'], axis=1)\n",
    "    else: \n",
    "        df_new = df_new.drop(['telecommuting', 'has_company_logo', 'has_questions', 'employment_type',\n",
    "       'required_experience', 'required_education', 'industry', 'function','title', 'location', 'department', 'salary_range', 'company_profile', 'description', 'requirements', 'benefits', 'in_balanced_dataset',\n",
    "                             'company_profile_count_chr', 'description_profile_count_chr', 'requirements_count_chr', 'benefits_count_chr'], axis=1)\n",
    "    return df_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f851777b-befe-4a35-8796-1eff6d0f56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_no_y = master_df.drop('fraudulent' ,axis =1)\n",
    "master_df['fraudulent']=master_df['fraudulent'].replace('f', 0)\n",
    "master_df['fraudulent']=master_df['fraudulent'].replace('t', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3f2f544a-280e-468d-8ebb-686f4d669e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(master_df_no_y, master_df['fraudulent'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "1ea1db13-1879-4afc-8d2e-af9d82522f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accruacy: 0.9515287099179717\n",
      "Test Stats\n",
      "Accuracy score: 0.9494407158836689\n",
      "Precision score: 0.0\n",
      "Recall score: 0.0\n",
      "F1 score: 0.0\n"
     ]
    }
   ],
   "source": [
    "train_X_fet = method_one_text_features(X_train, 'train')\n",
    "train_Y_fet = np.array(y_train)\n",
    "\n",
    "test_x = method_one_text_features(X_test, 'test')\n",
    "test_y = np.array(y_test)\n",
    "\n",
    "model_eda = LogisticRegression(max_iter=10000)\n",
    "model_eda.fit(train_X_fet, train_Y_fet)\n",
    "new_training_accuracy = model_eda.score(train_X_fet, train_Y_fet)\n",
    "\n",
    "\n",
    "print('Train Accruacy:', new_training_accuracy)\n",
    "pred = model_eda.predict(test_x)\n",
    "\n",
    "print('Test Stats')   \n",
    "print('Accuracy score:' , accuracy_score(pred, test_y))\n",
    "print('Precision score:', precision_score(pred, test_y))\n",
    "print ('Recall score:', recall_score(pred, test_y))\n",
    "print ('F1 score:', f1_score(pred, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9644a17-3441-4229-8fc0-211c84ef48c8",
   "metadata": {},
   "source": [
    "# Addtional Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5af0d8-e61d-43dc-94e6-d0e4a49c36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra\n",
    "arr_feature = ['work', 'experience', 'time', 'skills', 'amp', 'us', 'full', 'company', 'team', 'service', 'management', \n",
    "               'business', 'customer', 'ability', 'services', 'position', 'engineering', 'level', 'high', 'data', 'project',\n",
    "               'entry', 'industry', 'required', 'environment', 'new', 'must', 'solutions', 'years', 'job', 'support', 'development', \n",
    "               'products', 'knowledge', 'working', 'systems', 'looking', 'information', 'provide', 'office', 'within', 'benefits',\n",
    "               'candidates', 'people', 'product', 'requirements', 'sales', 'including', 'equipment', 'process', 'oil', 'communication', \n",
    "               'strong', 'technology', 'design', 'degree', 'customers', 'able', 'per', 'home', 'manager', 'training', 'quality', \n",
    "               'technical', 'false', 'good', 'professional', '1', 'opportunity', 'computer', 'school', '000', 'apply', 'develop',\n",
    "               'well', 'responsibilities', 'administrative', 'ensure', 'excellent', 'part', 'help', '2', 'system', 'field', 'employees',\n",
    "               'duties', 'perform', 'equivalent', 'get', 'please', 'client', 'world', 'responsible', 'gas', 'needed', 'test', 'operations', \n",
    "               'maintain', 'software', 'projects', 'production', 'preferred', 'ca', 'maintenance', 'related', 'positions', 'clients', \n",
    "               'offer', 'global', 'aker', 'contract', 'food', 'program', 'based', '3', 'start', 'bonus', 'paid']\n",
    "\n",
    "train_X = words_in_texts(arr_feature, df['text'])\n",
    "train_Y = np.array(df['fraudulent'])\n",
    "model_eda = LogisticRegression()\n",
    "model_eda.fit(train_X, train_Y)\n",
    "\n",
    "pred = model_eda.predict(train_X)\n",
    "new_training_accuracy = model_eda.score(train_X, train_Y)\n",
    "\n",
    "# print(\"Accuracy: \", new_training_accuracy)\n",
    "print('Accuracy score:' , accuracy_score(pred, train_Y))\n",
    "print('Precision score:', precision_score(pred, train_Y))\n",
    "print ('Recall score:', recall_score(pred, train_Y))\n",
    "print ('F1 score:', f1_score(pred, train_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f217a0-b39b-4d2d-9b2f-005910099be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = [50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]\n",
    "\n",
    "accuracy_score_lst = []\n",
    "precision_score_lst = []\n",
    "recall_score_lst = []\n",
    "f1_score_lst = []\n",
    "\n",
    "for num in num_words:\n",
    "    arr_feature = feature[:num]\n",
    "    \n",
    "    train_X = words_in_texts(arr_feature, X_train)\n",
    "    train_Y = np.array(y_train)\n",
    "    \n",
    "    test_x = words_in_texts(arr_feature, X_test)\n",
    "    test_y = np.array(y_test)\n",
    "    \n",
    "    model_eda = LogisticRegression(max_iter=10000)\n",
    "    model_eda.fit(train_X, train_Y)\n",
    "\n",
    "    pred = model_eda.predict(test_x)\n",
    "    new_training_accuracy = model_eda.score(train_X, train_Y)\n",
    "\n",
    "    # print(\"Accuracy: \", new_training_accuracy)\n",
    "    print(num, 'words')\n",
    "    print('Accuracy score:' , accuracy_score(pred, y_test))\n",
    "    print('Precision score:', precision_score(pred, y_test))\n",
    "    print ('Recall score:', recall_score(pred, y_test))\n",
    "    print ('F1 score:', f1_score(pred, y_test))\n",
    "    \n",
    "    accuracy_score_lst.append(accuracy_score(pred, y_test))\n",
    "    precision_score_lst.append(precision_score(pred, y_test))\n",
    "    recall_score_lst.append(recall_score(pred, y_test))\n",
    "    f1_score_lst.append(f1_score(pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026cc663-c551-49f4-b87d-374866726af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top n-number of words to test out.\n",
    "num_words = [50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]\n",
    "\n",
    "accuracy_score_lst = []\n",
    "precision_score_lst = []\n",
    "recall_score_lst = []\n",
    "f1_score_lst = []\n",
    "\n",
    "\n",
    "# loops over the number of words choices and train and test the model.\n",
    "for num in num_words:\n",
    "    arr_feature = feature[:num]\n",
    "    \n",
    "    train_X = words_in_texts(arr_feature, X_train)\n",
    "    train_Y = np.array(y_train)\n",
    "    \n",
    "    model_eda = LogisticRegression(max_iter=10000)\n",
    "    model_eda.fit(train_X, train_Y)\n",
    "\n",
    "    pred = model_eda.predict(X_test)\n",
    "    new_training_accuracy = model_eda.score(train_X, train_Y)\n",
    "\n",
    "    # print(\"Accuracy: \", new_training_accuracy)\n",
    "    print(num, 'words')\n",
    "    print('Accuracy score:' , accuracy_score(pred, train_Y))\n",
    "    print('Precision score:', precision_score(pred, train_Y))\n",
    "    print ('Recall score:', recall_score(pred, train_Y))\n",
    "    print ('F1 score:', f1_score(pred, train_Y))\n",
    "    \n",
    "    accuracy_score_lst.append(accuracy_score(pred, train_Y))\n",
    "    precision_score_lst.append(precision_score(pred, train_Y))\n",
    "    recall_score_lst.append(recall_score(pred, train_Y))\n",
    "    f1_score_lst.append(f1_score(pred, train_Y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
